[
  {
    "objectID": "OneShot/abstract.html",
    "href": "OneShot/abstract.html",
    "title": "Learned one-shot imaging",
    "section": "",
    "text": "\\[\n    \\newcommand{\\pluseq}{\\mathrel{+}=}\n\\]"
  },
  {
    "objectID": "OneShot/abstract.html#objectives-and-scope",
    "href": "OneShot/abstract.html#objectives-and-scope",
    "title": "Learned one-shot imaging",
    "section": "Objectives and scope",
    "text": "Objectives and scope\nSeismic imaging’s main limiting factor is the scale of the involved dataset and the number of independent wave-equation solves required to migrate thousands of shots. To tackle this dimensionality curse, we introduce a learned framework that extends the conventional computationally reductive linear source superposition (e.g., via random simultaneous-source encoding) to a nonlinear learned source superposition and its corresponding learned supershot. With this method, we can image the subsurface at the cost of a one-shot migration by learning the most informative superposition of shots."
  },
  {
    "objectID": "OneShot/abstract.html#method",
    "href": "OneShot/abstract.html#method",
    "title": "Learned one-shot imaging",
    "section": "Method",
    "text": "Method\nSimultaneous-source imaging takes advantage of the linearity of the wave equation by combining encoded (e.g., via random weights or time shifts) shot records together, reducing the number of wave-equation solves and therefore the cost of imaging. Because of the linearity, the same linear transformation can be applied to the sources to maintain source-shot consistency. To further reduce imaging costs, we propose using nonlinear encodings, and to compensate for the inability to apply the corresponding transform to the source, we simultaneously learn the nonlinear source and nonlinear data encodings with two deep networks. The first network takes shot records as input and outputs a learned supershot. The second network produces the associated nonlinearly encoded simultaneous source given this supershot. Lastly, we migrate the supershot using the nonlinear simultaneous source to obtain the one-shot seismic image. To jointly train these networks, we propose supervised and unsupervised formulations. In the supervised method, networks aim to minimize the difference between the predicted image and the ground-truth seismic image. Our unsupervised approach eliminates the need for true models, making the technique applicable in practice, by minimizing the difference between the observed data and the migrated-demigrated supershots. In both cases, the networks learn how to encode and insonify source-shot data pairs to extract maximal information on the image using a single super shot."
  },
  {
    "objectID": "OneShot/abstract.html#results",
    "href": "OneShot/abstract.html#results",
    "title": "Learned one-shot imaging",
    "section": "Results",
    "text": "Results\nWe apply the proposed one-shot imaging approach to a previously unseen shot data. The cost of one-shot imaging is approximately the same as a single shot migration as network evaluation costs are negligible. Results for the learned supervised and unsupervised one-shot imaging are presented in Figures 1 and 2, respectively. For reference, the original shot data are included on the left, followed by the learned supershot data and learned source. Corresponding images are plotted on the right. Figure 1 shows that the learned one-shot reverse-time migration (RTM) result constitutes a major improvement compared to the image obtained with conventional random source encoding (juxtapose images in the first column on the left). The one-shot imaging result is also better than convention sequential source RTM since it is closer to the true reflectivity plotted at the top of the right column. This improvement in the image’s frequency content can be explained by the fact that in the supervised learning formulation the the networks are jointly trained to produce broadband seismic images. Similarly, we find that similar observations are true for the unsupervised case, except that the improvement in image quality is slightly less pronounced. Besides being computationally inexpensive and providing high-fidelity images, since the source encoding is learned from data, our method requires no knowledge of the source signatures.\n\n\n\nFigure 1: Learned supershot and simultenous sources on a testing slice. We show a shot record and the migrated image with a random supershot and with sequential shots for reference.\n\n\n\n\n\nFigure 2: Unsupervised Learned supershot and simultenous sources on a testing slice. We show a shot record and the migrated image with a random supershot and with sequential shots for reference."
  },
  {
    "objectID": "OneShot/abstract.html#significance",
    "href": "OneShot/abstract.html#significance",
    "title": "Learned one-shot imaging",
    "section": "Significance",
    "text": "Significance\nWe introduced the first instance of learned nonlinear simultaneous-source encoding, enabling one-shot seismic imaging. After incurring initial training costs, the method is capable of producing high-fidelity images at the cost of migrating a single shot record, which entails a drastic reduction in migration costs. We introduced supervised and unsupervised training formulations, the latter of which only relies on a dataset of shot records from several seismic surveys, making it applicable in realistic settings. Additional material is available at https://slimgroup.github.io/IMAGE2023/."
  },
  {
    "objectID": "OneShot/abstract.html#introduction",
    "href": "OneShot/abstract.html#introduction",
    "title": "Learned one-shot imaging",
    "section": "Introduction",
    "text": "Introduction\n(Deans 2002) -Summary statistics reduce the size of incoming datasets while maintainting the same posterior distribution p(x|y) = p(x|summary) (Radev et al. 2020) -Summary networks learn to reduce the size of incoming datasets and maximize informativeness of the summarized data due to joint learning of summary network and posterior learning network. -hand waves an argument that jointly trained networks will maximize the mutual information between h(y) and x (Müller et al. 2021) -Goes in to further detail and rigoursly proves that jointly trained networks will maximize the mutual information between h(y) and x (Bloem-Reddy and Teh 2020) suggests to use learned layers that are invariant under a certain transformation. This transformation is described by the probabilistic assumption on your data."
  },
  {
    "objectID": "OneShot/abstract.html#methodology",
    "href": "OneShot/abstract.html#methodology",
    "title": "Learned one-shot imaging",
    "section": "Methodology",
    "text": "Methodology\nHere we present our formulation for a learned simultaneous source-data pair for seismic imaging. We propose two training formulations, one that relies on knowledge of the true perturbation (supervised), and the other that solely relies on the observed data (unsupervised). Fundamentally, we are introducing a formulation that learns a single supershot and corresponding source that maximally informs seismic imaging. Next section describes the design of the deep networks that enable this goal.\n\nSummary networks\nTo handle the high-dimensionality of seismic data and, we use summary networks (Radev et al. 2020) that are designed to exploit the intrinsic low-dimensional structure [ref needed to curvelet etc] of seismic data. These deep networks compress the observed seismic data \\(\\mathbf{d}\\) while maximally preserving the information useful for inferring the unknown seismic image \\(\\delta \\mathbf{m}\\). A major aspect of the architectural design of a summary networks are exploiting the invariances in data, e.g., invariance to permutation (Bloem-Reddy and Teh 2020). This is achieved by choosing neural architectures with functional forms that explicitly respect the intrinsic invariances in data. For example in the case of seismic imaging, the order of shot records has no effect on the final seismic image. To make this invariance explicit in the design of our network, we sum the shot records and feed it to a deep convolutional neural network.\n\n\nSupervised\nThe simplest formulation aims to learn the supershot and simultaneous source that best inform the model perturbation, given the surface recorded data. Mathematically, it means that we are trying to fit the true model perturbation with two networks that learn a single supershot and simultaneous source for the Jacobian from the individual field recorded shot records. Mathematically, the learning can be written as: \\[\n\\min_{\\boldsymbol{\\theta}, \\boldsymbol{\\phi}} \\ \\mathbb{E}_{(\\mathbf{d}, \\delta \\mathbf{m}) \\sim p(\\mathbf{d}, \\delta \\mathbf{m})} \\left[ \\frac{1}{2} \\Big\\| \\mathbf{J}\\big(\\mathcal{H}_{\\phi} \\circ \\mathcal{G}_{\\theta} \\left(\\mathbf{d}\\right)\\big)^\\top \\mathcal{G}_{\\theta}(\\mathbf{d}) - \\delta \\mathbf{m} \\Big\\|_2^2 \\right],\n\\tag{1}\\]\nwhere \\(\\mathcal{G}_{\\theta}\\) is the summary network that maps shot records to a learned, nonlinearly mixed supershot at the receiver locations, \\(\\mathcal{H}_{\\phi}\\) is the neural network responsible for estimating the associated nonlinearly encoded simultaneous source, and \\(\\circ\\) is the composition operator. The objective function matches the predicted seismic image, obtained by migrating the supershot \\(\\mathcal{G}_{\\theta}(\\mathbf{d})\\) via the adjoint Born imaging operator, \\(\\mathbf{J}\\), to the ground truth seismic image \\(\\delta \\mathbf{m}\\). We note that to evaluate the gradient of the objective function with jointly respect to \\(\\boldsymbol{\\theta}\\) and \\(\\boldsymbol{\\phi}\\), the gradient of the Jacobian with respect to its input (nonlinearly encoded source) is required. This derivative is, however, trivial to obtain with JUDI.jl thanks to its high-level linear algebra abstraction and integration with automatic differentiation framework in Julia.\n\n\nUnsupervised\nTo extend our approach to more realistic settings, we propose an unsupervised learning one-shot imaging approach that only requires a dataset of seismic shot records from several surveys. We formulate the problem as a minimization of the following objective function:\n\\[\n\\min_{\\boldsymbol{\\theta}, \\boldsymbol{\\phi}} \\ \\mathbb{E}_{\\mathbf{d} \\sim p(\\mathbf{d})} \\left[ \\frac{1}{2} \\Big\\|  \\tilde{\\mathbf{J}} \\mathbf{J}\\big(\\mathcal{H}_{\\phi} \\circ \\mathcal{G}_{\\theta}(\\mathbf{d})\\big)^\\top \\mathcal{G}_{\\theta}(\\mathbf{d}) - \\tilde{\\mathbf{d}} \\Big\\|_2^2  \\right ],\n\\tag{2}\\]\nwhere \\(\\tilde{\\mathbf{d}} = \\sum_{i=1}^{n_{\\text{src}}} w_i \\mathbf{d}_{i}\\) is a random super shot with \\(w_i := \\mathcal{N}(0, 1)\\) and \\(\\tilde{\\mathbf{J}}\\) is the corresponding simultaneous source born modeling operator. While this formulation ivolves an additional demigration (and therfore and additional migration to compute hte gradient), we do not require any knowledge of the true model perturbation but only the data. We could therefore in theory use this formulation for a wide range of datasets at once to generalize to any survey."
  },
  {
    "objectID": "OneShot/abstract.html#synthetic-case-studies",
    "href": "OneShot/abstract.html#synthetic-case-studies",
    "title": "Learned one-shot imaging",
    "section": "Synthetic case studies",
    "text": "Synthetic case studies\nWe illustrate our method on a realstic 2D imaging problem. We created a dataset of 2000 2D slices by extracting slices out of the 3D overthrust model. We then split this dataset into 1600 slices for trainng and 400 slices for testing. For each 2D slice, we generate 21 shot records. One of the main advantage of our one-shot imaging method is that we only require a single migration-demigration per iteration. Therefore, we can perform 21 epochs before arriving to a computationnal cost equivalent to the plain standard RTM imaging of each slice. Since we only perform 15 epochs, our method is overall cheapper than computing the RTM on every single shot if we include the cost of training.\nWe trained the networks, both in the supervised and unsupervised case, for 15 epochs with a learning rate of \\(.0004\\) using the Adam optimizer."
  },
  {
    "objectID": "OneShot/abstract.html#discussion-and-conclusions",
    "href": "OneShot/abstract.html#discussion-and-conclusions",
    "title": "Learned one-shot imaging",
    "section": "Discussion and conclusions",
    "text": "Discussion and conclusions\nWe introduced data-domain learning method that provides high accuracy imagies of the subsurface through one-shot imaging. We trained a network that learns the simultenous source and supershot that most inform the subsurface from the field recorded data. We showed that we obtain high accuracy images of the subsurface that contain broader frequency range than standard imaging and does not require prior knowledge of the source. Additionnally, the overall computationnal cost of training does not exceed the traditionnal cost of imaging.\n\nReferences\n\n\nBloem-Reddy, Benjamin, and Yee Whye Teh. 2020. “Probabilistic Symmetries and Invariant Neural Networks.” J. Mach. Learn. Res. 21: 90–91.\n\n\nDeans, Matthew C. 2002. “Maximally Informative Statistics for Localization and Mapping.” In Proceedings 2002 IEEE International Conference on Robotics and Automation (Cat. No. 02CH37292), 2:1824–29. IEEE.\n\n\nMüller, Jens, Robert Schmier, Lynton Ardizzone, Carsten Rother, and Ullrich Köthe. 2021. “Learning Robust Models Using the Principle of Independent Causal Mechanisms.” In DAGM German Conference on Pattern Recognition, 79–110. Springer.\n\n\nRadev, Stefan T, Ulf K Mertens, Andreas Voss, Lynton Ardizzone, and Ullrich Köthe. 2020. “BayesFlow: Learning Complex Stochastic Models with Invertible Neural Networks.” IEEE Transactions on Neural Networks and Learning Systems."
  },
  {
    "objectID": "OneShot/abstract.html#acknowledgement",
    "href": "OneShot/abstract.html#acknowledgement",
    "title": "Learned one-shot imaging",
    "section": "Acknowledgement",
    "text": "Acknowledgement\n\nThis research was carried out with the support of Georgia Research Alliance and partners of the ML4Seismic Center."
  },
  {
    "objectID": "OneShot/abstract.html#software-availability",
    "href": "OneShot/abstract.html#software-availability",
    "title": "Learned one-shot imaging",
    "section": "Software availability",
    "text": "Software availability\n\nResults presented here can be reproduced with the software and examples in OneShotImaging.jl distributed under MIT license."
  },
  {
    "objectID": "SequentialBayes/abstract.html",
    "href": "SequentialBayes/abstract.html",
    "title": "Monitoring Subsurface CO2 Plumes with Sequential Bayesian Inference",
    "section": "",
    "text": "\\[\n    \\newcommand{\\pluseq}{\\mathrel{+}=}\n\\]"
  },
  {
    "objectID": "SequentialBayes/abstract.html#objectives-and-scope-597600-characters",
    "href": "SequentialBayes/abstract.html#objectives-and-scope-597600-characters",
    "title": "Monitoring Subsurface CO2 Plumes with Sequential Bayesian Inference",
    "section": "OBJECTIVES AND SCOPE (597/600 characters)",
    "text": "OBJECTIVES AND SCOPE (597/600 characters)\nTo monitor and predict CO2 plume dynamics during geological carbon storage, reservoir engineers usually perform two-phase flow simulations. While these simulations may provide useful insights, their usefulness is limited due to numerous complicating factors including uncertainty in the dynamics of the plume itself. To study this phenomenon, we consider stochasticity in the dynamic caused by unknown random changes in the injection rate. By conditioning the CO2 plume predictions on seismic observations, we correct the CO2 plume predictions and quantify uncertainty with machine learning."
  },
  {
    "objectID": "SequentialBayes/abstract.html#methods-procedures-process-14431500-characters",
    "href": "SequentialBayes/abstract.html#methods-procedures-process-14431500-characters",
    "title": "Monitoring Subsurface CO2 Plumes with Sequential Bayesian Inference",
    "section": "METHODS, PROCEDURES, PROCESS (1443/1500 characters)",
    "text": "METHODS, PROCEDURES, PROCESS (1443/1500 characters)\nWe propose a method where at each timestep conditional neural networks are trained to generate samples of the CO2 saturation conditioned on current time-lapse seismic data. The method proceeds by generating training pairs consisting of the current CO2 saturation, which is simulated with a random injection rate from the previous timestep, and the associated reverse-time migration seismic image computed from simulated surface data. Given these training pairs, an amortized conditional normalizing flow (NF) is trained to approximate the posterior distribution for the saturation given seismic images. After training, the network is capable of drawing samples from the posterior distribution of the saturation given observed time-lapse seismic data at the current timestep. In turn, these samples for the CO2 saturation serve as input to two-phase flow simulations to predict the CO2 plume at the next timestep and the process repeats itself, recursively.\nCompared to more traditional data assimilation techniques, our approach has the distinct advantage that it does not make simplifying assumptions on the statistics and linearity of the CO2 plume dynamics and its coupling to time-lapse seismic data. The proposed method is also likelihood free. It only needs to be able to run simulations of the plume dynamics that may contain hidden stochastic terms. In our case, this stochastic term is the randomly varying injection rate.\n\n\n\n\n\n\n\n\n(a)\n\n\n\n\nFigure 1: Schematic of training process for a single timestep."
  },
  {
    "objectID": "SequentialBayes/abstract.html#results-observations-conclusions-11461500-characters",
    "href": "SequentialBayes/abstract.html#results-observations-conclusions-11461500-characters",
    "title": "Monitoring Subsurface CO2 Plumes with Sequential Bayesian Inference",
    "section": "RESULTS, OBSERVATIONS, CONCLUSIONS (1146/1500 characters)",
    "text": "RESULTS, OBSERVATIONS, CONCLUSIONS (1146/1500 characters)\nWe make CO2 plume predictions using both conditioned by seismic and unconditioned simulations. The conditional mean of the posterior samples is used for the seismic-conditioned predictions, while the unconditioned predictions are given by the ensemble mean over the different simulations at each timestep. Figure 2 shows that seismic-conditioned predictions are more accurate in predicting the ground truth and have a smaller conditional sample variance than the unconditioned ensemble mean predictions.\nBy examining the difference between the ground truth and predictions, we characterize the uncertainties. For the seismic-conditioned predictions, the uncertainties are located near the injection well. In addition, the plume’s boundary is well-predicted by the seismic-conditioned method while the unconditioned simulations have, as expected, an increasing uncertainty at the boundary over time and tend to consistently over-estimate the plume extent.\nOverall, we conclude that the unconditioned simulation poorly predicts the CO2 plume. Our seismic-conditioned framework assimilates information and learns over time, which improves the CO2 forecasting and characterizes uncertainties.\n\n\n\n\n\n\n\n(a)\n\n\n\n\nFigure 2: Results from our method compared with unconditional ensemble variogram."
  },
  {
    "objectID": "SequentialBayes/abstract.html#significancenovelty-238600-characters",
    "href": "SequentialBayes/abstract.html#significancenovelty-238600-characters",
    "title": "Monitoring Subsurface CO2 Plumes with Sequential Bayesian Inference",
    "section": "SIGNIFICANCE/NOVELTY (238/600 characters)",
    "text": "SIGNIFICANCE/NOVELTY (238/600 characters)\nTo our understanding, the seismic-conditioned framework is the first geological carbon storage monitoring model that continuously assimilates information with seismic observations, updates the CO2 plume predictions, and characterizes uncertainties on the plume itself. Because or prediction are conditioned by the physics, we expect this method to extend to larger and more realistic model sources of uncertainty (permeability, topology, …)."
  },
  {
    "objectID": "DetectabilityWithVision/abstract.html",
    "href": "DetectabilityWithVision/abstract.html",
    "title": "Analysis of CO2 Leakage Detectability via Vision Classifier",
    "section": "",
    "text": "In previous work, we have shown that neural classifiers can be trained to detect CO2 leakage from time-lapse seismic images. While such result is key to the global deployment of geological carbon storage (GCS), it success depended on relatively dense non-replicated time-lapse data acquisition. We will show that by augmenting the training set with various coarse receiver samplings, we can improve the CO2 leakage detection capabilities and accuracy while increasing the robustness with respect to low-cost coarse non-replicated receiver samplings, e.g. with ocean bottom nodes (OBNs)."
  },
  {
    "objectID": "DetectabilityWithVision/abstract.html#methods-procedures-process-250-words-1500",
    "href": "DetectabilityWithVision/abstract.html#methods-procedures-process-250-words-1500",
    "title": "Analysis of CO2 Leakage Detectability via Vision Classifier",
    "section": "Methods, Procedures, Process (250 words, 1500)",
    "text": "Methods, Procedures, Process (250 words, 1500)\nTo create a suitable training set, we derive permeability models from 2D proxy models with a geology representative for potential GCS sites in the South of the North Sea. These models are derived from 3D imaged seismic and well-log data and contain realistic heterogeneity. Given these permeability models, we create a training set of seismically imaged CO2 plumes that behave regularly (without leakage) and irregularly (with leakage). After generating the baseline and monitor surveys, time-lapse seismic images of the plume are created with reverse time migration (RTM). The CO2 plumes themselves are modeled with two-phase flow equations.\nTo create high-fidelity images, we deploy 162 receivers and 32 jittered sources {ML: I’d put the sampling in meters, this could be anything}. Given these images we train our visual classifier based on a Vision Transformer network. As we can observe from Figure 1(a), this classifier meets our ultimate purpose of distinguishing between regular and irregular plumes by performing binary classification. While encouraging the training and test images all rely on high-fidelity imaging obtained with densely sampled OBNs. Unfortunately, in practice we can not rely on dense receiver sampling because of the associated high costs and the classifier performance on seismic images obtained form coarse receiver sampling drastically deteriorates (Figure 1(b)). To overcome this problem, we augment the training dataset with time-lapse seismic images computed from a plurality of acquisition geometries with varying coarse receiver samplings.\n\n\n\n\n\n\n\n(a)\n\n\n\n\nFigure 1: Workflow."
  },
  {
    "objectID": "DetectabilityWithVision/abstract.html#results-observations-conclusions-250-words-1500",
    "href": "DetectabilityWithVision/abstract.html#results-observations-conclusions-250-words-1500",
    "title": "Analysis of CO2 Leakage Detectability via Vision Classifier",
    "section": "Results, Observations, Conclusions (250 words, 1500)",
    "text": "Results, Observations, Conclusions (250 words, 1500)\nFrom Figure 2(a), we observe that the testing accuracy of the network trained initially on 162 receiver setting performs well for acquisition geometries with more receivers but its accuracy degrades gradually when the number of receivers becomes smaller rendering the classifier untrustworthy for less 100 receivers. On the other hand, we see in Figure (2a) that the network trained with the augmented dataset containing images from coarse receiver sampling provides higher accuracy for all receiver geometries and better robustness to coarse acquisition. Moreover, the confusion matrix performance metrics (precision, recall and F-1 score) also improved substantially showing the improved trustworthiness new network\n\nThese results show that dataset augmentation can indeed enhance the generalization of our neural classifier and contributes to its leakage detection accuracy. In terms of practical usage, this work suggests a dataset based on a particular seismic time-lapse acquisition geometry can easily be made applicable to different geometries by adding time-lapse seismic images obtained with complementary random acquisition geometries to the training dataset.\n\n\n\n\n\n\n\n(a)\n\n\n\n\nFigure 2: Classification Results."
  },
  {
    "objectID": "DetectabilityWithVision/abstract.html#significancenovelty-100-words-600",
    "href": "DetectabilityWithVision/abstract.html#significancenovelty-100-words-600",
    "title": "Analysis of CO2 Leakage Detectability via Vision Classifier",
    "section": "Significance/Novelty (100 words, 600)",
    "text": "Significance/Novelty (100 words, 600)\nTo our knowledge, this is a first study how to make neural classifiers for CO2 leakage detection more robust with respect to variations in receiver geometry and density. To achieve this, we propose a simple dataset augmentation technique that improves generalization of the network to changes in the acquisition geometry. The results suggest that our study has the potential to lower dataset collection costs while training classifiers capable of automatic leakage detection from time-lapse seismic images computed from non-replicated surveys with varying coarse receiver density."
  },
  {
    "objectID": "BayesianKrig/abstract.html",
    "href": "BayesianKrig/abstract.html",
    "title": "Generative Seismic Kriging with Normalizing Flows",
    "section": "",
    "text": "\\[\n    \\newcommand{\\pluseq}{\\mathrel{+}=}\n\\]"
  },
  {
    "objectID": "BayesianKrig/abstract.html#objectives-and-scope-581600-characters",
    "href": "BayesianKrig/abstract.html#objectives-and-scope-581600-characters",
    "title": "Generative Seismic Kriging with Normalizing Flows",
    "section": "OBJECTIVES AND SCOPE (581/600 characters)",
    "text": "OBJECTIVES AND SCOPE (581/600 characters)\nThe objective is to demonstrate the applicability of Normalizing Flows (NFs) to subsurface kriging from wells. We will show that after supervised training, we can generate multiple realistic samples of plausible earth models that match the observed wells. We observe that these samples produce uncertainty statistics that are correlated with the complex parts of the model. The applicability of this method is for areas nearby the original survey used for training. Finally, we compare the speed and quality of our solutions with those obtained using a traditional variogram approach."
  },
  {
    "objectID": "BayesianKrig/abstract.html#methods-procedures-process-15011500-characters",
    "href": "BayesianKrig/abstract.html#methods-procedures-process-15011500-characters",
    "title": "Generative Seismic Kriging with Normalizing Flows",
    "section": "METHODS, PROCEDURES, PROCESS (1501/1500 characters)",
    "text": "METHODS, PROCEDURES, PROCESS (1501/1500 characters)\nKriging is highly ill-posed (there is no unique solution), therefore, ideal methods should be able to produce many models that match the observed well log data. Generative networks can be used to sample models that are conditioned on observations. A particular class of generative networks are normalizing flows. These are particularly attractive because they are fast to sample from and have low training memory requirements from their invertibility. We implemented our architecture in Julia with InvertibleNetworks.jl.\nOur method is supervised and needs training examples of observed wells “y” with its corresponding full earth models “x”. We use the 3D Compass model with a 90%/5%/5% train/validate/test split of 2D slices. For each training slice (nz=256, nx=512, d=10m) of the Compass volume, we randomly generate well observations by selecting 5 horizontal locations at least 200 meters from each other. This process creates the training pairs (x_i,y_i) for the normalizing flow. We made 10k pairs and trained as visualized in Figure 1. (TODO: ADD AXIS TO ONE OF THE PLOTS HERE)\nAfter training, we input an unseen well log y (5 wells) and produce posterior samples p(x|y) (earth models). To create a single point estimate, we average all posterior samples to get the posterior mean.\nAs a baseline, we use exponential variogram from the package PyKrige. The variogram parameters are automatically selected by the well log data. We manually set the anisotropy angle to 0 to match the overall horizontally layered structures of Compass."
  },
  {
    "objectID": "BayesianKrig/abstract.html#results-observations-conclusions-15281500-characters",
    "href": "BayesianKrig/abstract.html#results-observations-conclusions-15281500-characters",
    "title": "Generative Seismic Kriging with Normalizing Flows",
    "section": "RESULTS, OBSERVATIONS, CONCLUSIONS (1528/1500 characters)",
    "text": "RESULTS, OBSERVATIONS, CONCLUSIONS (1528/1500 characters)\nIn Figure 2, we show posterior samples generated with our method (each sample takes 10ms to compute on a single GPU). To validate the quality of the earth models produced, we compare the posterior mean with the known ground truth from a leave-out test set. We compare various metrics (SSIM, PSNR, RSME and time-to-compute) and verify that our method produces better reconstructions while being faster than a variogram approach.\nWe study the uncertainty of our approach by calculating the variance between the posterior samples and compare with the calculated variance of the variogram. We see in Figure 2 that our method produces uncertainty results that are more interpretable and correlate with specific structures in the subsurface.\nThe table in Figure 2 shows the quantitative performance of our method. Our method produces a high quality point estimates with an average RMSE of 0.038 compared to the variogram with an average RMSE of 0.043. The posterior mean gives a smooth model with less error on average, however we recommend practitioners use posterior samples as they maintain realistic earth characteristics.\nThis method has learned long range structures in the training survey area and can extrapolate them further. Generalization to other survey areas is out of the scope of this project but future work will explore it. We conclude that our method is a promising option for creating realistic earth models that match observed data wells and that it offers quantitative advantages over traditional approaches."
  },
  {
    "objectID": "BayesianKrig/abstract.html#significancenovelty-657600-characters",
    "href": "BayesianKrig/abstract.html#significancenovelty-657600-characters",
    "title": "Generative Seismic Kriging with Normalizing Flows",
    "section": "SIGNIFICANCE/NOVELTY (657/600 characters)",
    "text": "SIGNIFICANCE/NOVELTY (657/600 characters)\nWe introduce the first use of conditional normalizing flows for kriging. While previous implementations of conditional normalizing flows have struggled on high dimensional models, we demonstrated our software implementation allows for learning distributions over large realistic models. This method is set to scale to 3D models in future work which will further enable the application of these methods to real-world seismic problems. In contrast with traditional variograms, our method produces realistic samples, that is particularly important for downstream tasks in reservoir engineering and other applications where multiple plausible models are needed.\n\n\n\n\n\n\n\nfig1\n\n\n\n\nFigure 1: Schematic of full training process and test time evaluation of our method.\n\n\n\n\n\n\n\n\n\nfig2\n\n\n\n\nFigure 2: Results from our method compared with exponential variogram. Our method produces realistic samples of earth models that when averaged produce high quality point estimates."
  },
  {
    "objectID": "license.html",
    "href": "license.html",
    "title": "License",
    "section": "",
    "text": "These abstracts are released under the Creative Commons License type CC BY. 2023 Copyrights (c) SLIM Group, Georgia Institute of Technology.\nAll codes used to produce these results are released under MIT license."
  },
  {
    "objectID": "BayesianFWI/abstract.html",
    "href": "BayesianFWI/abstract.html",
    "title": "Amortized Bayesian Full Waveform Inversion and Experimental Design with Normalizing Flows",
    "section": "",
    "text": "Probabilistic approaches to Full-Waveform Inversion (FWI), such as Bayesian ones, traditionally require expensive computations involving many wave-equation solves. To reduce the computational burden at test time, we propose to amortize the computational cost with offline training. After training, we aim to efficiently generate probabilistic FWI solutions with uncertainty. This aim is achieved by exploiting the ability of networks (i.e Normalizing Flows) to learn distributions, such as the Bayesian posterior. The posterior uncertainty is used during training to optimize the receiver sampling.\n\n\n\nNormalizing flows (NFs) are generative networks that can learn to sample from conditional distributions, here our desired Bayesian posterior p(x|y) where x are earth models and y is seismic data. To train NFs, we require training pairs of those earth models and seismic data. We use pairs from the open source dataset OPENFWI. The earth models are .7km by .7km and the seismic data is simulated with 5 sources with 15Hz frequency and 20dB noise. We use the CurveFault_B dataset from OPENFWI that contains 55k pairs and split the pairs for training, validation, and testing 90%/5%/5% respectively.\nTo learn to sample from the conditional distribution, the network learns to transform the earth models to Gaussian normal noise while conditioned on the acoustic data. By construction the network is invertible, thus after training we sample Gaussian normal noise and apply the inverse network to generate samples from the posterior, again conditioned by the seismic data. The training and testing processes are illustrated schematically in Figure 1.\nThe NF training objective has been shown to be equivalent to maximizing the information gain, as defined by Bayesian optimal experimental design (EOD), of the conditioned data. Thus, we propose to jointly optimize for a receiver sampling mask M and the network parameters during NF training to simultaneously learn the Bayesian posterior and the optimal minimal receiver geometry.\n\n\n\n\n\n\n\n(a)\n\n\n\n\nFigure 1: Schematic of full training process and test time evaluation of our method.\n\n\n\n\n\nAfter training, our method generates posterior samples at the cost of one neural network inverse pass (10ms on our GPU). Seen in Figure 2, the posterior samples are realistic earth models that could plausibly correspond to the seismic data. The variations between the posterior samples are due to the FWI problem being ill-posed and because of noise in the indirect observations at the surface (20dB noise). To study these variations, we take the sample variance as our uncertainty quantification (UQ). We observe that the UQ is concentrated on steep and deeper structures. Both these types of structures are difficult to invert for with FWI showing that our UQ correlates with the expected error making our UQ realistic and interpretable.\nTo make a single high-quality solution, we take the mean of the posterior samples. This posterior mean x_{pm} shows high-quality metrics that surpass previous OPENFWI benchmarks while producing Bayesian uncertainty not available with the OPENFWI benchmarks.\nThe optimal receiver sampling obtained with our method (black circles in “test” portion of Figure 1) are the traces that the network has learned to select in order to inform the inverse problem.\nOur method is amortized since the previously mentioned computational debt associated with probabilistic FWI is paid by modeling the training pairs (x,y) and the cost of training the network. After these costs are incurred offline, our method can efficiently sample from the Bayesian posterior for many unseen seismic datasets without wave-equation solves.\n\n\n\n\n\n\n\n(a)\n\n\n\n\nFigure 2: Results from our method. Our method produces realistic samples of earth models that when averaged produce high quality point estimates. The variance in posterior samples correlates with structures that are difficult to invert for in FWI.\n\n\n\n\n\nTo our knowledge, this is the first demonstration of conditional normalizing flows applied to amortized FWI with Bayesian uncertainty quantification. We also showed a practical application of the uncertainty towards optimal experimental design of acquisition geometry. While the open source dataset used has small models, NFs are memory efficient due to their intrinsic invertibility and are set to scale to realistic sized problems and field datasets."
  },
  {
    "objectID": "yin2023IMAGEend2end/abstract.html",
    "href": "yin2023IMAGEend2end/abstract.html",
    "title": "Coupled physics inversion for geological carbon storage monitoring",
    "section": "",
    "text": "Understanding CO2 plume behavior is key to the success of geological carbon storage projects. While two-phase flow equations provide a good model to make predictions on future CO2 plume behavior, these equations rely on having access to the true permeability model. Unfortunately, accurate information on the permeability is unavailable, which greatly jeopardizes our ability to predict CO2 plume behavior. To overcome this problem, we estimate the permeability from time-lapse seismic data via a coupled inversion methodology that improve as more monitoring data becomes available over time."
  },
  {
    "objectID": "yin2023IMAGEend2end/abstract.html#methods-procedures-process-1500-characters",
    "href": "yin2023IMAGEend2end/abstract.html#methods-procedures-process-1500-characters",
    "title": "Coupled physics inversion for geological carbon storage monitoring",
    "section": "Methods, Procedures, Process (1500 characters)",
    "text": "Methods, Procedures, Process (1500 characters)\nTo estimate the spatial permeability distribution, we adopt a coupled physics inversion framework that involves three kinds of physics, namely two-phase fluid-flow, rock, and wave physics. The fluid-flow equations model the time evolution of the CO2 concentration and pressure given the permeability distribution. The rock physics modeling converts the time-varying CO2 concentration into the acoustic wavespeed. Finally, seismic modeling generates time-lapse seismic data for each vintage based on the wavespeed of the rocks. A schematic of this multiphysics forward model is shown in Figure 1.\nGiven observed time-lapse seismic data, we then directly invert for the permeability through the fully nested physics modeling operators. The inverted permeability can be used to generate time-varying CO2 concentration snapshots that match observed time-lapse seismic data. Aside from obtaining estimates for the CO2 plume’s past and current behavior, constrained by the two-phase flow equations, the proposed inversion methodology is also capable of producing reliable CO2 forecasts from the inverted permeability. These forecasts can be produced for different injection scenarios allowing for in-situ interventions designed to optimize productivity and minimize risks."
  },
  {
    "objectID": "yin2023IMAGEend2end/abstract.html#results-observations-conclusions-1500-characters",
    "href": "yin2023IMAGEend2end/abstract.html#results-observations-conclusions-1500-characters",
    "title": "Coupled physics inversion for geological carbon storage monitoring",
    "section": "Results, Observations, Conclusions (1500 characters)",
    "text": "Results, Observations, Conclusions (1500 characters)\nWe conduct a realistic numerical study based on the North Sea Compass model whose geology is very similar to sites currently being considered as a potential site for geological carbon storage. We convert the wavespeed in the model to log-permeability values to make up alternating high- and low-permeability layers in the reservoir with a seal on top. We inject CO2 in a highly permeable layer for 25 years. During and after injection, the CO2 tends to move into high permeability layers and move up due to buoyancy effects. To monitor the CO2 plume seismically, we shoot 5 surveys of crosswell data every 5th year, using a Ricker wavelet with a central frequency of 20 Hz.\nWe start the inversion with homogenous permeability values in the reservoir. After 10 data passes of gradient descent, the CO2 plume recovery from the inverted permeability is shown in Figure 2(a). The extent of the plume looks drastically different from the initial prediction based on the homogeneous permeability, but reasonably similar to the ground truth CO2 plumes. As expected, we only obtain information on the permeability from regions within the CO2 plume during the first 25 years of injection, shown in Figure 2(b). While the inverted permeability captures only part of the true permeability model, it improves drastically on plume forecasts for the next 25 years compared to those obtained from the starting model for the permeability. More information is in https://github.com/slimgroup/IMAGE2023."
  },
  {
    "objectID": "yin2023IMAGEend2end/abstract.html#significancenovelty-600-characters",
    "href": "yin2023IMAGEend2end/abstract.html#significancenovelty-600-characters",
    "title": "Coupled physics inversion for geological carbon storage monitoring",
    "section": "Significance/Novelty (600 characters)",
    "text": "Significance/Novelty (600 characters)\nTo our knowledge, this is the first numerical study that applies a multiphysics inversion framework to a realistic geological carbon storage site. The fluid-flow simulations and sensitivity calculations are conducted using state-of-the-art solvers in JutulDarcy.jl, which accounts for capillary effects and residual trapping (purple colors). Also, the simulations are based on a proxy permeability model derived from real imaged seismic and well data. Experiment shows that the proposed methodology can be applied to geological carbon storage projects to estimate and forecast CO2 plume evolution.\n\n\n\n\n\n\n\nfig1\n\n\n\n\nFigure 1: Figure 1\n\n\n\n\n\n\n\n\n\nfig2\n\n\n\n\nFigure 2: Figure 2"
  },
  {
    "objectID": "zhang2023IMAGEsg/abstract.html",
    "href": "zhang2023IMAGEsg/abstract.html",
    "title": "3D seismic survey design by maximizing the spectral gap",
    "section": "",
    "text": "The massive cost of 3D acquisition calls for methods to reduce the number of receivers by designing optimal receiver sampling masks. Recent studies on 2D seismic showed that maximizing the spectral gap of the subsampling mask leads to better wavefield reconstruction results. We enrich the current study by proposing a simulation-free method to generate optimal 3D acquisition by maximizing the spectral gap of the subsampling mask via a simulated annealing algorithm. Numerical experiments confirm improvement of the proposed method over receiver sampling locations obtained by jittered sampling."
  },
  {
    "objectID": "zhang2023IMAGEsg/abstract.html#methods-procedures-process-1500-characters",
    "href": "zhang2023IMAGEsg/abstract.html#methods-procedures-process-1500-characters",
    "title": "3D seismic survey design by maximizing the spectral gap",
    "section": "Methods, Procedures, Process (1500 characters)",
    "text": "Methods, Procedures, Process (1500 characters)\nThe spectral gap ratio is the ratio of the first and second singular values of a binary subsampling mask (binary matrix). It is a cheap-to-compute measure to predict wavefield reconstruction quality from acquisition geometry only. Motivated by recent success on 2D survey design methods driven by spectral gap ratio minimization, we consider 3D survey design where receivers are missing and sources are fully sampled. Because 3D wavefield reconstruction based on low-rank matrix completion relies on the non-canonical Source-X/Receiver-X (columns) Source-Y/Receiver-Y (rows) organization of the data into a matrix, we aim to minimize the spectral gap ratio of the subsampling mask in that domain. Fortunately, when sources are fully sampled, each single-receiver block of the global sampling matrix is either fully sampled or empty depending on whether that specific receiver is sampled. Consequently, the block structure of the global matrix leads to the exact same singular values as a single-source receiver sampling mask. We can therefore optimize a single-source mask to obtain the global optimized mask. The main computational cost is computing the first two singular values of the receiver sampling mask, which is negligible compared to approaches that require wave simulations. The resulting optimal mask with the lowest spectral gap ratio indicates the receiver sampling locations that favor 3D wavefield reconstruction via matrix completion in the non-cononical organization domain."
  },
  {
    "objectID": "zhang2023IMAGEsg/abstract.html#results-observations-conclusions-1500-characters",
    "href": "zhang2023IMAGEsg/abstract.html#results-observations-conclusions-1500-characters",
    "title": "3D seismic survey design by maximizing the spectral gap",
    "section": "Results, Observations, Conclusions (1500 characters)",
    "text": "Results, Observations, Conclusions (1500 characters)\nTo illustrate the efficacy of our method via a numerical experiment on a simulated 3D marine dataset over the compass model. The data volume consists of 501 time samples, 1681 sources and 10k receivers. The distance between the adjacent sources and receivers are 150m and 25m in each direction, respectively, with a time sampling interval of 0.01s. By removing 90% of receivers using jittered subsampling, we obtain a binary matrix with the spectral gap ratio 0.507 in the non-canonical domain. After applying simulated annealing algorithm, the spectral gap ratio of mask effectively decreases to 0.328. To validate the efficacy of our acquisition design method, we perform data reconstruction on a frequency slice at 16.8Hz via weighted matrix completion for the two subsampled datasets with jittered subsampling mask and the proposed mask, with results shown in Figure 2. The reconstruction signal-to-noise ratio from the observed data at proposed receiver locations is 12.27 dB, which is about 1.4 dB higher than the reconstruction signal-to-noise ratio 10.88 dB achieved from data observed at jittered sampled receiver locations. This confirms that the proposed optimized receiver sampling locations result in a superior seismic survey that leads to better wavefield reconstruction performance. More information is in https://github.com/slimgroup/IMAGE2023."
  },
  {
    "objectID": "zhang2023IMAGEsg/abstract.html#significancenovelty-600-characters",
    "href": "zhang2023IMAGEsg/abstract.html#significancenovelty-600-characters",
    "title": "3D seismic survey design by maximizing the spectral gap",
    "section": "Significance/Novelty (600 characters)",
    "text": "Significance/Novelty (600 characters)\nThis is the first numerical case study that applies spectral gap ratio minimization techniques to seismic acquisition design that favors 3D wavefield reconstruction. Rather than requiring costly wave simulations, the proposed method only relies on a single binary matrix optimization which is computationally inexpensive. Experiments demonstrate that the proposed method generates an improved 3D seismic survey better suitable for 3D wavefield reconstruction.\n\n\n\n\n\n\n\nfig1\n\n\n\n\nFigure 1: Figure 1\n\n\n\n\n\n\n\n\n\nfig2\n\n\n\n\nFigure 2: Figure 2"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Image2023",
    "section": "",
    "text": "This is a Quarto website.\nAll submissions to the Image23 conference with additional figures, references, …\nList of abstracts:\n\nLearned one-shot imaging Learned non-linear simultenous source and corresponding supershot for seismic imaging.\nSequential Bayesian Inference Plume Monitoring with Learned Sequential Bayesian Inference\nCoupled inversion Coupled physics inversion for geological carbon storage monitoring.\nBayesian FWI Amortized Bayesian Full Waveform Inversion and Experimental Design with Normalizing Flows\nBayesian Kriging Generative Seismic Kriging with Normalizing Flows\nDetectability with Vision Classification Analysis of CO2 Leakage Detectability via Vision Classifier\n3D survey design 3D seismic survey design by maximizing the spectral gap\nNonLinear JRM Time-lapse seismic monitoring of geological carbon storage using non-linear joint recovery model."
  },
  {
    "objectID": "NonLinear-JRM/abstract.html",
    "href": "NonLinear-JRM/abstract.html",
    "title": "Time-lapse seismic monitoring of geological carbon storage with the nonlinear joint recovery model",
    "section": "",
    "text": "During time-lapse seismic monitoring, weak 4D signal below the level of inversion or migration artifacts poses challenges. To address these, low-cost randomized non-replicated acquisitions and linear joint recovery model (JRM) have been introduced to take advantage of the shared information between different vintages in the time-lapse seismic data and subsurface structure undergoing localized changes. Since the relationship between seismic data and subsurface properties is seldomly linear, we propose a more versatile nonlinear JRM, which extends linear JRM to nonlinear forward modeling."
  },
  {
    "objectID": "NonLinear-JRM/abstract.html#methods-procedures-process-250-words-1500",
    "href": "NonLinear-JRM/abstract.html#methods-procedures-process-250-words-1500",
    "title": "Time-lapse seismic monitoring of geological carbon storage with the nonlinear joint recovery model",
    "section": "Methods, Procedures, Process (250 words, 1500)",
    "text": "Methods, Procedures, Process (250 words, 1500)\nDuring linear JRM, for two seismic vintages, three unknown parameters are inverted, namely the common component, \\(m_0\\), and innovation components, \\(\\delta m_1\\) and \\(\\delta m_2\\), with respect to this common component and there is a linear relationship between these components and seismic data. But during the proposed nonlinear JRM (nJRM), we take advantage of the full nonlinear relation between these components and time-lapse data resulting in minimization of the objective included in Figure 1. After mathematical manipulations on the expression for the gradients, the computational costs of minimizing this joint objective can be made equivalent to independent FWI (one gradient per vintage).\n\nTo demonstrate the advocacy of the proposed nonlinear extension, we consider an experimental setup involving CO2 monitoring. To model a time-lapse change in the monitor survey, we simulated two-phase flow in a 2D slice from the Compass model (See Figure XXX)  followed by simulating time-lapse seismic data for the baseline and monitor surveys."
  },
  {
    "objectID": "NonLinear-JRM/abstract.html#results-observations-conclusions-250-words-1500",
    "href": "NonLinear-JRM/abstract.html#results-observations-conclusions-250-words-1500",
    "title": "Time-lapse seismic monitoring of geological carbon storage with the nonlinear joint recovery model",
    "section": "Results, Observations, Conclusions (250 words, 1500)",
    "text": "Results, Observations, Conclusions (250 words, 1500)\nFigures 1 (a) and (b) show independent and joint recovery respectively for the replicated acquisition and Figures 2 (a) and (b) for the non-replicated acquisition. In these figures, the first and second rows show recovery of baseline and monitor images respectively, the third and fourth rows show inverted and ground truth time-lapse differences. In both cases, time-lapse differences inverted with nonlinear JRM are more accurate and contain less artifacts and noise thanks to exploiting common information between surveys. Moreover, independent recovery suffers consequently from non-replication with clear artifacts in the shallow area (Figure 2(a)) while nonlinear JRM stays relatively focused on the time-lapse difference.\n\nThese results strengthens our argument for a low-cost randomized non-replicated acquisition in the time-lapse seismic survey for GCS.\nFinally, we measure the degree of repeatability of the recovery of the vintages and their time-lapse difference which is mathematically determined by the normalized root mean square (NRMS) value where a lower value indicates better recovery. In this study, the nrms value for the JRM in the case of the replicated and non-replicated surveys are 0.5% and 1.2% respectively and the SNR value between the ground truth and recovered time-lapse is -2.02 dB and -7.1 dB respectively."
  },
  {
    "objectID": "NonLinear-JRM/abstract.html#significancenovelty-100-words-600",
    "href": "NonLinear-JRM/abstract.html#significancenovelty-100-words-600",
    "title": "Time-lapse seismic monitoring of geological carbon storage with the nonlinear joint recovery model",
    "section": "Significance/Novelty (100 words, 600)",
    "text": "Significance/Novelty (100 words, 600)\nTo our knowledge, this is the first study on the application of non-linear JRM with non-replicated time-lapse data for geological carbon storage.  With the proposed new technique, we argue that there is no need for a replicated expensive time-lapse survey, rather non-replicated coarser surveys are sufficient to provide an artifact-free time-lapse signal with JRM. More information on this work is in (https://github.com/slimgroup/IMAGE2023).\n\n\n\n\n\n\n\n\nfig1\n\n\n\n\nFigure 1: Figure 1\n\n\n\n\n\n\n\n\n\nfig2\n\n\n\n\nFigure 2: Figure 2"
  }
]