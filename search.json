[
  {
    "objectID": "zhang2023IMAGEsg/abstract.html",
    "href": "zhang2023IMAGEsg/abstract.html",
    "title": "3D seismic survey design by maximizing the spectral gap",
    "section": "",
    "text": "The huge cost of 3D seismic acquisition calls for methods to reduce the number of receivers in the acquisition by designing optimal receiver sampling masks. Recent studies on 2D seismic show that maximizing the spectral gap of the subsampling mask leads to better wavefield reconstruction results according to expander graph theory. We enrich the current study by proposing a simulation-free method to automatically generate optimal 3D seismic acquisition by maximizing the spectral gap of the subsampling mask via a simulated annealing algorithm. Numerical experiments confirm improvement of the proposed acquisition design method over receiver sampling locations obtained by jittered sampling."
  },
  {
    "objectID": "zhang2023IMAGEsg/abstract.html#methods-procedures-process-250-words",
    "href": "zhang2023IMAGEsg/abstract.html#methods-procedures-process-250-words",
    "title": "3D seismic survey design by maximizing the spectral gap",
    "section": "Methods, Procedures, Process (250 words)",
    "text": "Methods, Procedures, Process (250 words)\nThe spectral gap ratio is the ratio of the first and second singular values of a binary subsampling mask. It is a cheap-to-compute measure to predict wavefield reconstruction quality from binary sampling masks. Motivated by recent success on 2D seismic survey design methods driven by spectral gap ratio minimization, we consider 3D seismic survey design where receivers are missing and sources are fully sampled. We propose a 3D seismic survey design method via minimizing the spectral gap ratio of 3D source sampling mask.\nBecause 3D wavefield reconstruction based on low-rank matrix completion relies on the non-canonical Source-X/Receiver-X (columns) Source Y/Receiver Y (rowws) organization of the data into a matrix, we aim to minimize the spectral gap ratio of subsampling mask in that domain. Fortunately, when sources are fully sampled, each subsampled receiver location becomes a fully sampled square in this mask. As a result, the spectral gap ratio in the non-canonical domain is exactly the same as the spectral gap ratio in the common-shot domain (see Figure 1). Therefore, we implement the simulated annealing algorithm to iteratively find receiver subsampling masks that minimize the spectral gap ratio in the common-shot domain. The main computational cost of this algorithm is computing the first two singular values of the source subsampling mask, which is negligible compared to approaches that require wave simulations. The resulting optimal mask with the lowest spectral gap ratio indicates the receiver sampling locations that favor 3D wavefield reconstruction via matrix completion in the non-cononical organization."
  },
  {
    "objectID": "zhang2023IMAGEsg/abstract.html#results-observations-conclusions-250-words",
    "href": "zhang2023IMAGEsg/abstract.html#results-observations-conclusions-250-words",
    "title": "3D seismic survey design by maximizing the spectral gap",
    "section": "Results, Observations, Conclusions (250 words)",
    "text": "Results, Observations, Conclusions (250 words)\nTo illustrate the efficacy of our method via a numerical experiment on a simulated 3D marine dataset over the compass model. The data volume consists of \\(501 \\times 100 \\times 100 \\times 41 \\times 41\\) entries—i.e., \\(n_t \\times n_{rx} \\times n_{ry} \\times n_{sx} \\times n_{sy}\\) along the time, receiver \\(x\\), receiver \\(y\\), source \\(x\\), and source \\(y\\) directions. The distance between the adjacent sources and receivers are 150m and 25m, respectively, with a time sampling interval of 0.01s. By removing 90% of receivers using jittered subsampling, we obtain a binary matrix with the spectral gap ratio 0.507 in the non-canonical domain. After applying simulated annealing algorithm, the spectral gap ratio of mask effectively decreases to 0.328. To validate the efficacy of our acquisition design method, we perform data reconstruction on a frequency slice at 16.8Hz via weighted matrix completion for the two subsampled datasets with jittered subsampling mask and the proposed mask, with results shown in Figure 2. The reconstruction signal-to-noise ratio from the observed data at proposed source locations is 12.27 dB, which is about 1.4 dB higher than the reconstruction signal-to-noise ratio 10.88 dB achieved from data oserved at jittered sampled source locations. This confirms that the proposed optimized sources sampling locations result in a superior seismic survey that leads to better wavefield reconstruction performance."
  },
  {
    "objectID": "zhang2023IMAGEsg/abstract.html#significancenovelty-100-words",
    "href": "zhang2023IMAGEsg/abstract.html#significancenovelty-100-words",
    "title": "3D seismic survey design by maximizing the spectral gap",
    "section": "Significance/Novelty (100 words)",
    "text": "Significance/Novelty (100 words)\nThis is the first numerical case study that applies spectral gap ratio minimization techniques to seismic acquisition design that favors 3D wavefield reconstruction. Rather than requiring costly wave simulations, the proposed method for optimizing binary masks is computationally inexpensive. Experiments demonstrate that the proposed method generates an improved 3D seismic survey better suitable for 3D wavefield reconstruction.\n\n\n\n\n\n\n\nfig1\n\n\n\n\nFigure 1: Figure 1\n\n\n\n\n\n\n\n\n\nfig2\n\n\n\n\nFigure 2: Figure 2"
  },
  {
    "objectID": "OneShot/abstract.html",
    "href": "OneShot/abstract.html",
    "title": "Learned one-shot imaging",
    "section": "",
    "text": "\\[\n    \\newcommand{\\pluseq}{\\mathrel{+}=}\n\\]"
  },
  {
    "objectID": "OneShot/abstract.html#objectives-and-scope-600-characters",
    "href": "OneShot/abstract.html#objectives-and-scope-600-characters",
    "title": "Learned one-shot imaging",
    "section": "OBJECTIVES AND SCOPE (600 characters)",
    "text": "OBJECTIVES AND SCOPE (600 characters)\nSeismic imaging’s main limiting factor is the scale of the involved dataset and the number of independent wave-equation solves required to migrate thousands of shots. To tackle this dimensionality curse, we introduce a learned framework that extends the conventional computationally reductive linear source superposition (e.g., via random simultaneous-source encoding) to a nonlinear learned source superposition and its corresponding learned supershot. With this method, we can image the subsurface at the cost of a one-shot migration by learning the most informative superposition of shots."
  },
  {
    "objectID": "OneShot/abstract.html#methods-procedures-process-1500-characters",
    "href": "OneShot/abstract.html#methods-procedures-process-1500-characters",
    "title": "Learned one-shot imaging",
    "section": "METHODS, PROCEDURES, PROCESS (1500 characters)",
    "text": "METHODS, PROCEDURES, PROCESS (1500 characters)\nSimultaneous-source imaging takes advantage of the linearity of the wave equation by combining encoded (e.g., via random weights or time shifts) shot records together, reducing the number of wave-equation solves and therefore the cost of imaging. Because of the linearity, the same linear transformation can be applied to the sources to maintain source-shot consistency. To further reduce imaging costs, we propose using nonlinear encodings, and to compensate for the inability to apply the corresponding transform to the source, we simultaneously learn the nonlinear source and nonlinear data encodings with two neural networks. The first network takes shot records as input and outputs a learned supershot. The second network produces the associated nonlinearly encoded simultaneous source given this supershot. Lastly, we migrate the supershot using the nonlinear simultaneous source to obtain the seismic image. To jointly train these networks, we propose supervised and unsupervised formulations. In the supervised method, networks aim to minimize the difference between the predicted image and the true reflectivity, which requires access to the true model. To alleviate the need to have access to true models, our unsupervised method minimizes the difference between the observed shot data and the migrated-demigrated supershots. In both cases, the networks learn how to encode and insonify source-shot data pairs to extract maximal information on the image."
  },
  {
    "objectID": "OneShot/abstract.html#results-observations-conclusions-1500-characters",
    "href": "OneShot/abstract.html#results-observations-conclusions-1500-characters",
    "title": "Learned one-shot imaging",
    "section": "RESULTS, OBSERVATIONS, CONCLUSIONS (1500 characters)",
    "text": "RESULTS, OBSERVATIONS, CONCLUSIONS (1500 characters)\n\nResults for the learned supervised and unsupervised one-shot imaging are included in Figures 1 and 2, respectively. For reference, the original shot data are included on the left, followed by the learned supershot data and learned source. Corresponding images are plotted on the right. We make the following observations from the results. First, Figure 1 shows that the learned one-shot reverse-time migration (RTM) result constitutes a major improvement compared to the image obtained with conventional random source encoding (juxtapose images in the first column on the left). Second, the one-shot imaging result is also better than convention sequential source RTM since it is closer to the true reflectivity plotted at the top of the right column. This improvement in the image’s frequency content can be explained by the fact that in the supervised case the networks are trained to broadband reflectivities. Third, for the unsupervised case we find that similar observations hold except that the improvement in image quality is slightly less. Because the encoded source distribution is learned, our method does not need information on the source signatures."
  },
  {
    "objectID": "OneShot/abstract.html#significancenovelty-600-characters",
    "href": "OneShot/abstract.html#significancenovelty-600-characters",
    "title": "Learned one-shot imaging",
    "section": "SIGNIFICANCE/NOVELTY (600 characters)",
    "text": "SIGNIFICANCE/NOVELTY (600 characters)\nWe introduced a first instance of a new one-shot nonlinear simultenaous source imaging paradigm. After incurring initial training costs, the method is capable of producing high-fidelity images at the cost of migrating single shot records, which entails a drastic reduction in migration costs. Training of the networks is also relatively cheap because it also involves single-source operations."
  },
  {
    "objectID": "OneShot/abstract.html#introduction",
    "href": "OneShot/abstract.html#introduction",
    "title": "Learned one-shot imaging",
    "section": "Introduction",
    "text": "Introduction"
  },
  {
    "objectID": "OneShot/abstract.html#methodology",
    "href": "OneShot/abstract.html#methodology",
    "title": "Learned one-shot imaging",
    "section": "Methodology",
    "text": "Methodology\nWe now introduce the formulation of aour learned simultenous source-data pair for seismic imaging. We derive two training problem where the first onbe rely on the knowledge of the true perturbation, while the second one solely rely on the observed data. Fundamentally, we are introducing a formulation that learns the most informative single super-shot and correspoinding source given either a subsurface refelctivity model or the surface recorded data.\n\nSummary networks and probabilistic symmetry\n(Deans 2002) -Summary statistics reduce the size of incoming datasets while maintainting the same posterior distribution p(x|y) = p(x|summary) (Radev et al. 2020) -Summary networks learn to reduce the size of incoming datasets and maximize informativeness of the summarized data due to joint learning of summary network and posterior learning network. -hand waves an argument that jointly trained networks will maximize the mutual information between h(y) and x (Müller et al. 2021) -Goes in to further detail and rigoursly proves that jointly trained networks will maximize the mutual information between h(y) and x (Bloem-Reddy and Teh 2020) suggests to use learned layers that are invariant under a certain transformation. This transformation is described by the probabilistic assumption on your data.\nExample paragraph: This work takes inspiration from the concept of a summary network (Radev et al. 2020) these are networks that compress observables \\(d_{obs}\\) while maximining information useful for inference of un-observables \\(x\\). To guide the architectural design of a summary network (Bloem-Reddy and Teh 2020) suggests to use learned layers that respect the probabalistic symmetry of the data. Practically, this is accomplished by making the layers be invariant under a certain transformation. For the case active source seismic imaging, i.i.d sampling entails the assumption that the order of sources does not matter. This assumption is implicit in the sum structure of RTM/gradient calculations. Therefore it is invariant with respect to permutation transformation. Our approach is most similar to this case, since we use a UNet with many input channels that output a single channel. In our testing, having a separate network for each dataset did not perform better thatn simply inputing each dataset along a channel.\n\n\nSupervised\nTHe simplest formulation aims to learn the super-shot and simultenaous source that best inform the model perturbation, given the surfac recorded data. Mathematically, it means that we are trying to fit the true mode lperturbation with two networks that learn a single super-shot and simultenous source for the Jacobian from the indicudual field recorded shot records. Mathematically, the learning can be written as: \\[\n\\min_{\\theta, \\phi} \\ \\mathbb{E}\\left[ J(\\mathcal{H}_{\\phi}(\\mathcal{G}_{\\theta}(d_{\\text{obs}}))^\\top \\mathcal{G}_{\\theta}(d_{\\text{obs}}) - \\delta m \\right]\n\\tag{1}\\]\nwhere \\(\\mathcal{H}_{\\phi}, \\mathcal{G}_{\\theta}\\) are the two networks mapping the individual shot records into a single super-shot (the learned simultenous-source is learned at the receiver locations), \\(J\\) is the conventionnal adjoint Born imaging operator, \\(d_{\\text{obs}}\\) is the observed data and \\(\\delta m\\) is the model perturbation. We note that to compute an update on the two networks simultenaously, the gradient of the Jacobian with respect to its source is necessary. This derivative is however trivial to obtain with JUDI.jl thanks to its high-level linear algebra abstraction and integration with automatic differentiation framework in Julia.\n\n\nUnsupervised\n\\[\n\\min_{\\theta, \\phi} \\ \\mathbb{E}\\left[ \\tilde{J}_{\\text{rtm}} J(\\mathcal{H}_{\\phi}(\\mathcal{G}_{\\theta}(d_{\\text{obs}}))^\\top \\mathcal{G}_{\\theta}(d_{\\text{obs}}) - \\tilde{d}_{\\text{obs}} \\right ]\n\\tag{2}\\]\nWhere \\(\\tilde{d}_{\\text{obs}} = \\sum_{i=1}^{n_src} w_i d_{\\text{obs},i}\\) is a random super shot with \\(w_i := \\mathcal{N}(0, 1)\\) and \\(\\tilde{J}\\) is the corresponding simultenous source born modeling operator. While this formulation ivolves an additional demigration (and therfore and additional migration to compute hte gradient), we do not require any knowledge of the true model perturbation but only the data. We could therefore in theory use this formulation for a wide range of datasets at once to generalize to any survey."
  },
  {
    "objectID": "OneShot/abstract.html#synthetic-case-studies",
    "href": "OneShot/abstract.html#synthetic-case-studies",
    "title": "Learned one-shot imaging",
    "section": "Synthetic case studies",
    "text": "Synthetic case studies\nWe illustrate our method on a realstic 2D imaging problem. We created a dataset of 2000 2D slices by extracting slices out of the 3D overthrust model. We then split this dataset into 1600 slices for trainng and 400 slices for testing. For each 2D slice, we generate 21 shot records. One of the main advantage of our one-shot imaging method is that we only require a single migration-demigration per iteration. Therefore, we can perform 21 epochs before arriving to a computationnal cost equivalent to the plain standard RTM imaging of each slice. Since we only perform 15 epochs, our method is overall cheapper than computing the RTM on every single shot if we include the cost of training.\nWe trained the networks, both in the supervised and unsupervised case, for 15 epochs with a learning rate of \\(.0004\\) using the Adam optimizer.\n\n\n\n\n\n\n\nData\n\n\n\n\n\n\n\nRTMs\n\n\n\n\nFigure 1: Learned super-shot and simultenous sources on a testing slice. We show a shot record and the moirated image with a random supershot and with sequential shots for reference.\n\n\n\n\n\n\n\n\n\nData\n\n\n\n\n\n\n\nRTMs\n\n\n\n\nFigure 2: Learned super-shot and simultenous sources on a testing slice. We show a shot record and the moirated image with a random supershot and with sequential shots for reference.\n\n\n\nCode availability\nResults presented here can be reproduced with the software and examples in OneShotImaging.jl."
  },
  {
    "objectID": "OneShot/abstract.html#discussion-and-conclusions",
    "href": "OneShot/abstract.html#discussion-and-conclusions",
    "title": "Learned one-shot imaging",
    "section": "Discussion and conclusions",
    "text": "Discussion and conclusions\nWe introduced data-domain learning method that provides high accuracy imagies of the subsurface through one-shot imaging. We trained a network that learns the simultenous source and super-shot that most inform the subsurface from the field recorded data. We showed that we obtain high accuracy images of the subsurface that contain broader frequency range than standard imaging and does not require prior knowledge of the source. Additionnally, the overall computationnal cost of training does not exceed the traditionnal cost of imaging."
  },
  {
    "objectID": "OneShot/abstract.html#acknowledgement",
    "href": "OneShot/abstract.html#acknowledgement",
    "title": "Learned one-shot imaging",
    "section": "Acknowledgement",
    "text": "Acknowledgement\nThis research was carried out with the support of Georgia Research Alliance and partners of the ML4Seismic Center.\n\nReferences\n\n\nBloem-Reddy, Benjamin, and Yee Whye Teh. 2020. “Probabilistic Symmetries and Invariant Neural Networks.” J. Mach. Learn. Res. 21: 90–91.\n\n\nDeans, Matthew C. 2002. “Maximally Informative Statistics for Localization and Mapping.” In Proceedings 2002 IEEE International Conference on Robotics and Automation (Cat. No. 02CH37292), 2:1824–29. IEEE.\n\n\nMüller, Jens, Robert Schmier, Lynton Ardizzone, Carsten Rother, and Ullrich Köthe. 2021. “Learning Robust Models Using the Principle of Independent Causal Mechanisms.” In DAGM German Conference on Pattern Recognition, 79–110. Springer.\n\n\nRadev, Stefan T, Ulf K Mertens, Andreas Voss, Lynton Ardizzone, and Ullrich Köthe. 2020. “BayesFlow: Learning Complex Stochastic Models with Invertible Neural Networks.” IEEE Transactions on Neural Networks and Learning Systems."
  },
  {
    "objectID": "license.html",
    "href": "license.html",
    "title": "License",
    "section": "",
    "text": "These abstracts are released under the Creative Commons License type CC BY. Copyrights (c) SLIM Group, Georgia Institute of Technology.\nAll codes used to produce these results are released under MIT license,."
  },
  {
    "objectID": "BayesianKrig/abstract.html",
    "href": "BayesianKrig/abstract.html",
    "title": "Generative Seismic Kriging with Normalizing Flows",
    "section": "",
    "text": "\\[\n    \\newcommand{\\pluseq}{\\mathrel{+}=}\n\\]"
  },
  {
    "objectID": "BayesianKrig/abstract.html#objectives-and-scope-581600-characters",
    "href": "BayesianKrig/abstract.html#objectives-and-scope-581600-characters",
    "title": "Generative Seismic Kriging with Normalizing Flows",
    "section": "OBJECTIVES AND SCOPE (581/600 characters)",
    "text": "OBJECTIVES AND SCOPE (581/600 characters)\nThe objective is to demonstrate Normalizing Flows (NFs) for subsurface kriging from wells. We will show that after supervised training of our method, we can generate multiple realistic samples of plausible earth models that match the observed wells. We observe that these samples produce uncertainty statistics that are correlated with the error made by the method. The applicability of this method is for areas nearby the original survey area used for training. Finally, we compare the speed and quality of our solutions with those obtained using a traditional variogram approach."
  },
  {
    "objectID": "BayesianKrig/abstract.html#methods-procedures-process-14061500-characters",
    "href": "BayesianKrig/abstract.html#methods-procedures-process-14061500-characters",
    "title": "Generative Seismic Kriging with Normalizing Flows",
    "section": "METHODS, PROCEDURES, PROCESS (1406/1500 characters)",
    "text": "METHODS, PROCEDURES, PROCESS (1406/1500 characters)\nKriging is highly ill-posed (there is no unique solution) so the preferred method should be able to produce many models that match the observed well log data. Generative deep learning can be used to sample models that are conditioned on observations. A particular class of generative deep learning are normalizing flows. These are particularly attractive because they are fast to sample from and they have low training memory requirements. We implemented an architecture in Julia with InvertibleNetworks.jl.\nOur method is supervised so it needs training examples of observed wells y where the corresponding earth models x. We use the compass model with a 90%/5%/5% training/validation/test split. For each training slice (nz=256, nx=512, d=10m) of the compass volume, we randomly generate well observations by selecting 5 columns at least 200 meters distance between each. This process creates the training pairs (x_i,y_i) used to train the conditional normalizing flow.\nAfter training, we input an unseen well log y and produce samples of the posterior p(x|y). To create single point estimate, we average all posterior samples to get the posterior mean.\nFor the variogram, we use exponential ordinary variogram from the package PyKrige. The variogram parameters are automatically selected by the well log data. We manually set the anisotropy angle to 0 to match the strongly horizontal Compass model."
  },
  {
    "objectID": "BayesianKrig/abstract.html#results-observations-conclusions-16651500-characters",
    "href": "BayesianKrig/abstract.html#results-observations-conclusions-16651500-characters",
    "title": "Generative Seismic Kriging with Normalizing Flows",
    "section": "RESULTS, OBSERVATIONS, CONCLUSIONS (1665/1500 characters)",
    "text": "RESULTS, OBSERVATIONS, CONCLUSIONS (1665/1500 characters)\nIn Figure 2, we show posterior samples from our method where each sample takes 10ms to compute. To validate the quality of the earth models produced by our method, we compare the posterior mean with known ground truth models from a leave-out test set. We compare various metrics (SSIM, PSNR, RSME and time-to-compute and verify that our method produces higher quality reconstructions while being faster than a variogram approach.\nWe study the uncertainty of our approach by looking at the variation between the posterior samples. We compare our posterior variance with the calculated standard variation of the variogram. Compared against the variograms variance our method produces uncertainty results that are more interpretable and correlate with errors made in specific structures.\nThe table in Figure 2 shows the quantitative performance of our method, in summary, our method takes less than 2 seconds to produce a high quality point estimates with an average RMSE of 0.038 compared to the variogram with an average RMSE of 0.043. The posterior mean gives less error on average, but the earth models are smoothed thus for downstream tasks, we recommend practitioners use posterior samples as they maintain realistic earth characteristics.\nThis method has learned long range structures in the training survey area and can extrapolate them further. Generalization to other survey areas is out of the scope of this project but future work will explore this. We conclude that our method is a promising option for creating realistic earth models that match observed data wells and that it offers quantitative advantages over traditional approaches."
  },
  {
    "objectID": "BayesianKrig/abstract.html#significancenovelty-663600-characters",
    "href": "BayesianKrig/abstract.html#significancenovelty-663600-characters",
    "title": "Generative Seismic Kriging with Normalizing Flows",
    "section": "SIGNIFICANCE/NOVELTY (663/600 characters)",
    "text": "SIGNIFICANCE/NOVELTY (663/600 characters)\nWe introduce the first use of conditional normalizing flows for kriging. While previous implementations of conditional normalizing flows have struggled on high dimensional images, we demonstrated our software implementation allows for learning distributions over large images. This method is set to scale to 3D image volumes in future work which will further enable the application of these methods to realistic seismic problems. In contrast with traditional variogram methods, our framework produces realistic samples, that is particularly important for downstream tasks in reservoir engineering and other applications where multiple plausible models are needed.\n\n\n\n\n\n\n\nfig1\n\n\n\n\nFigure 1: Schematic of full training process and test time evaluation of our method.\n\n\n\n\n\n\n\n\n\nfig2\n\n\n\n\nFigure 2: Results from our method compared with exponential variogram. Our method produces realistic samples of earth models that when averaged produce high quality point estimates."
  },
  {
    "objectID": "yin2023IMAGEend2end/abstract.html",
    "href": "yin2023IMAGEend2end/abstract.html",
    "title": "Coupled physics inversion for geological carbon storage monitoring",
    "section": "",
    "text": "Understanding CO2 plume behavior is key to the success of geological carbon storage projects. While two-phase flow equations provide a good model to make predictions on future CO2 plume behavior, these equations rely on having access to the spatial permeability distribution. Unfortunately, accurate information on the permeability distribution is unavailable, which greatly jeopardizes our ability to predict CO2 plume behavior. To overcome this problem, we estimate the permeability from time-lapse seismic data via a coupled inversion methodology that is capable of producing CO2 plume predictions that improve as more seismic monitoring data becomes available."
  },
  {
    "objectID": "yin2023IMAGEend2end/abstract.html#methods-procedures-process-250-words",
    "href": "yin2023IMAGEend2end/abstract.html#methods-procedures-process-250-words",
    "title": "Coupled physics inversion for geological carbon storage monitoring",
    "section": "Methods, Procedures, Process (250 words)",
    "text": "Methods, Procedures, Process (250 words)\nTo obtain information on the spatial permeability distribution, we adopt coupled physics inversion framework that involves three kinds of physics, namely two-phase fluid-flow, rock, and wave physics. The fluid-flow equations model the time evolution of the CO2 saturation and pressure given the permeability distribution. The rock physics modeling turns the time-varying CO2 saturation into time-varying changes of the acoustic wavespeed. Finally, seismic modeling generates time-lapse seismic data for each vintage based on the wavespeed of the rocks. A schematic of this multiphysics forward model is shown in Figure 1.\nGiven observed time-lapse seismic data, we invert the three nested physics modeling operators for the permeability. The inverted permeability can be used to generate time-varying CO2 concentration snapshots that match observed time-lapse seismic data. Aside from obtaining estimates for the CO2 plume’s past and current behavior, constrained by the two-phase flow equations, the proposed inversion methodology is thanks to the inverted permeability also capable of producing CO2 forecasts. These forecasts can be produced for different injection scenarios allowing for interventions designed to optimize productivity and minimize risks."
  },
  {
    "objectID": "yin2023IMAGEend2end/abstract.html#results-observations-conclusions-250-words",
    "href": "yin2023IMAGEend2end/abstract.html#results-observations-conclusions-250-words",
    "title": "Coupled physics inversion for geological carbon storage monitoring",
    "section": "Results, Observations, Conclusions (250 words)",
    "text": "Results, Observations, Conclusions (250 words)\nTo gain insight on the performance of our inversion scheme, we conduct a realistic numerical study based on the North Sea Compass model whose geology is very similar to sites currently being considered as a potential site for geological carbon storage. For this purpose, we convert the compressional wavespeed in the model to log permeability values to make up alternating high- and low-permeability layers in the reservoir with a seal on top. We inject 1 million metric ton CO2 per year in a highly permeable layer for 25 years. During and after injection, the CO2 tends to move into high permeability layers (over 1000 millidarcies) and move up due to buoyancy effects. To monitor the CO2 plume seismically, we shoot 5 surveys of crosswell data every 5th year, using a Ricker wavelet with a central frequency of 20 Hz.\nWe start the inversion with homogenous permeability values in the reservoir. After 12 data passes of gradient descent with back-tracking line search, the CO2 plume recovery from the inverted permeability is shown in Figure 2 (a). The extent of the plume looks drastically different from the initial prediction based on the homogeneous permeability, but reasonably similar to the ground truth CO2 plumes simulated from the true unknown permeability distribution.  As expected, we only obtain information on the permeability from regions that are touched by the CO2 plume during the first 25 years of injection. While the inverted permeability captures only part of the true permeability distribution, it improves drastically on plume forecasts for the next 25 years compared to those obtained from the starting model for the permeability."
  },
  {
    "objectID": "yin2023IMAGEend2end/abstract.html#significancenovelty-100-words",
    "href": "yin2023IMAGEend2end/abstract.html#significancenovelty-100-words",
    "title": "Coupled physics inversion for geological carbon storage monitoring",
    "section": "Significance/Novelty (100 words)",
    "text": "Significance/Novelty (100 words)\nTo our knowledge, this is a first numerical study where a multiphysics inversion framework is applied to a realistic geological carbon storage site. Not only are the fluid-flow simulations based on a proxy model for the permeability derived from real imaged seismic and well data but the simulations, including sensitivity calculations, are carried out with a state-of-the art solvers ([JutulDarcy.jl]), which accounts for capillary effects and residual trapping (purple colors).  From this example, we observe that the proposed inversion methodology can be applied to geological carbon storage projects to estimate and forecast CO2 plume evolution.\n\n\n\n\n\n\n\nfig1\n\n\n\n\nFigure 1: Figure 1\n\n\n\n\n\n\n\n\n\nfig2\n\n\n\n\nFigure 2: Figure 2"
  },
  {
    "objectID": "SequentialBayes/abstract.html",
    "href": "SequentialBayes/abstract.html",
    "title": "Monitoring Subsurface CO2 Plume with Sequential Bayesian Inference",
    "section": "",
    "text": "\\[\n    \\newcommand{\\pluseq}{\\mathrel{+}=}\n\\]"
  },
  {
    "objectID": "SequentialBayes/abstract.html#objectives-and-scope-667600-characters",
    "href": "SequentialBayes/abstract.html#objectives-and-scope-667600-characters",
    "title": "Monitoring Subsurface CO2 Plume with Sequential Bayesian Inference",
    "section": "OBJECTIVES AND SCOPE (667/600 characters)",
    "text": "OBJECTIVES AND SCOPE (667/600 characters)\nTo monitor and predict the CO2 dynamics for geological carbon storage, reservoir engineers usually perform fluid-flow simulations using the existing Earth subsurface profiles. However, without considering the stochasticity caused by the constantly changing Earth subsurface and the injection process, CO2 plume predictions can be wrong. We demonstrate a framework using sequential Bayesian inference to simulate the time-evolving CO2 plume with the stochasticity induced by the varying injection rate. By conditioning the CO2 plume predictions on seismic observations, we show a more accurate prediction of the CO2 plume with the uncertainty characterized."
  },
  {
    "objectID": "SequentialBayes/abstract.html#methods-procedures-process-18551500-characters",
    "href": "SequentialBayes/abstract.html#methods-procedures-process-18551500-characters",
    "title": "Monitoring Subsurface CO2 Plume with Sequential Bayesian Inference",
    "section": "METHODS, PROCEDURES, PROCESS (1855/1500 characters)",
    "text": "METHODS, PROCEDURES, PROCESS (1855/1500 characters)\nWe generate the prior x_0, a dataset with 128 CO2 saturation data (x) each with a size of 64x64 . Each prior data has a circular initial CO2 plume with a uniformed CO2 saturation s N(0.2,0.8). Besides the prior, two physics models, the transition model (M) and seismic observation model (H), are used for all experiments. The stochastic transition model to calculate the evolution of CO2 saturation data over each timestep with fluid flow physics with a randomized injection rate I N(0,0.2). The seismic model made observations (y) based on the simulated acoustic measurements at the surface.\nThe ground truth is generated by randomly sampling one CO2 saturation data (x^) from the prior and transition over five timesteps. Seismic observations (^) are made at each timestep. Two numerical experiments are performed to predict the ground truth: unconditioned simulation and seismic-conditioned simulation.\nThe unconditioned simulation is performed to represent the current CO2 plume forecasting technique. The entire prior dataset is transitioned and seismically observed for five timesteps.\nOur seismic-conditioned simulation involves four main processes: transitioning, seismic observation, Conditional Normalizing Flow (CNF) training, and posterior sampling (Figure 1). After the prior (x_{n}) is transitioned and seismically observed, the (x_{n+1},y_{n+1}) are the training pair for Conditional Normalizing Flow, a generative network that can learn to sample from conditional distributions p(x|y). Therefore, we sample from the conditioned CNF-generated posterior on the round truth observation x_{n+1} p(x|y=y_{n+1}^. For sequential Bayesian inference, the posterior samples are served as the prior of the next timestep, so {n+1} p(x|y=y{n+1}^)) is transitioned to x_{n+2}."
  },
  {
    "objectID": "SequentialBayes/abstract.html#results-observations-conclusions-10731500-characters",
    "href": "SequentialBayes/abstract.html#results-observations-conclusions-10731500-characters",
    "title": "Monitoring Subsurface CO2 Plume with Sequential Bayesian Inference",
    "section": "RESULTS, OBSERVATIONS, CONCLUSIONS (1073/1500 characters)",
    "text": "RESULTS, OBSERVATIONS, CONCLUSIONS (1073/1500 characters)\nCO2 plume predictions are made using both conditioned seismic and unconditioned simulations. The seismic-conditioned predictions are the conditional mean of the posterior samples, while the unconditioned predictions are the ensemble mean at each timestep. Figure 2 shows that seismic-conditioned predictions are more accurate in predicting the ground truth and have a smaller conditional sample variance than the unconditioned ensemble mean predictions.\nThe uncertainties can be characterized by examining the difference between the ground truth and predictions. For the seismic-conditioned predictions, the uncertainties are located near the injection well. The boundary of the plume is well-predicted by the seismic-conditioned method while the unconditioned simulation has an increasing uncertainty at the boundary over time.\nOverall, we conclude that the unconditioned simulation poorly predicts the CO2 plume. Our seismic-conditioned framework assimilates information and learns over time, which improves the CO2 forecasting and characterizes uncertainties."
  },
  {
    "objectID": "SequentialBayes/abstract.html#significancenovelty-238600-characters",
    "href": "SequentialBayes/abstract.html#significancenovelty-238600-characters",
    "title": "Monitoring Subsurface CO2 Plume with Sequential Bayesian Inference",
    "section": "SIGNIFICANCE/NOVELTY (238/600 characters)",
    "text": "SIGNIFICANCE/NOVELTY (238/600 characters)\nTo our understanding, the seismic-conditioned framework is the first geological carbon storage monitoring model that assimilates information with seismic observations, updates the CO2 plume predictions, and characterizes uncertainties."
  },
  {
    "objectID": "BayesianFWI/abstract.html",
    "href": "BayesianFWI/abstract.html",
    "title": "Amortized Bayesian Full Waveform Inversion and Experimental Design with Normalizing Flows",
    "section": "",
    "text": "Probabilistic approaches to Full-Waveform Inversion (FWI), such as Bayesian ones, traditionally require expensive computations involving multiple wave-equation solves. To reduce this computational burden at test time, we propose to amortize the computational cost with offline training. After training, our method aims to efficiently generate probabilistic FWI solutions with uncertainty. This aim is achieved by exploiting the ability of deep networks (i.e. Normalizing Flows) to learn distributions, in our case, the Bayesian posterior. The posterior uncertainty is used during training to find the receiver optimal experimental design (OED).\n\n\n\nNormalizing flows (NFs) are generative networks that can learn to sample from conditional distributions, i.e. our desired Bayesian posterior p(x|y) where x are earth models and y is seismic data. To train NFs, we require training pairs of these earth models and seismic data. We use pairs from the open source dataset OPENFWI. The earth models are 64x64 size and the seismic data is simulated with 5 source experiments with 15Hz frequency and 20dB noise. We use the CurveFault_B dataset from OPENFWI that contains 55k pairs. We use a 90%/5%/5% split for training, validation, and testing.\nTo learn to sample from the conditional distribution, the network learns to transform the earth models to Gaussian normal noise while conditioned on the acoustic data. By construction the network is invertible, thus after training we resample Gaussian normal noise and pass it through the inverse network to generate samples from the posterior. The training process is illustrated schematically in Figure 1.\nThe NFs training objective has been shown to be equivalent to maximizing the information gain (as defined by Bayesian OED) of the conditioned data. Thus, we propose to jointly optimize a mask M that occludes receivers of the seismic data (M.*y) during NF training. Our optimization jointly optimizes for NF parameters and optimal mask M values in a single objective (Equation 1 in Figure 1).\n\n\n\nAfter training, our method generates posterior samples at the cost of one neural network pass (10ms on our GPU). Seen in Figure 2, the posterior samples are realistic earth models that could plausibly match the seismic data. The variations between the models are due to the FWI problem being ill-posed and because of noise in the indirect observations at the subsurface. To study these variations, we take the sample variance as our uncertainty quantification. We note that the uncertainty is concentrated on vertical and deeper structures. Both these structures are difficult to image with FWI leading to the pleasing result that our uncertainty correlates with error made making our uncertainty useful and interpretable.\nTo make a single high-quality solution, we take the mean of the posterior samples. The posterior mean shows high-quality metrics that surpass previous benchmarks on the OPENFWI dataset while our method also produces Bayesian uncertainty and the benchmark methods do not.\nThe OED found by our method (filled in circles in Figure 2 represent optimal receiver locations) matches prior expectations that the most important information is contained in the short offsets. This result was independently discovered by the Bayesian optimization routine.\nOur method is amortized since the previously mentioned computational debt associated with probabilistic FWI is paid by the forward modeling needed to compute training pairs (x,y) plus the cost of training the network. After both these costs are incurred offline, our method can efficiently sample from the Bayesian posterior for many unseen seismic datasets.\n\n\n\nTo our knowledge, this is the first demonstration of conditional normalizing flows on amortized FWI with uncertainty quantification. We also showed a practical application of the uncertainty towards experimental design. While the open source dataset has small models, NFs are memory efficient due to their intrinsic invertibility so are set to scale to realistic sized problems."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Image2023",
    "section": "",
    "text": "This is a Quarto website.\nAll submissions to the Image23 conference with additional figures, references, …\nList of abstracts:\n\nLearned one-shot imaging Learned non-linear simultenous source and corresponding supershot for seismic imaging.\nSequential Bayesian Inference Plume Monitoring with Learned Sequential Bayesian Inference\nCoupled inversion Coupled physics inversion for geological carbon storage monitoring.\nBayesian FWI Amortized Bayesian Full Waveform Inversion and Experimental Design with Normalizing Flows\nBayesian Kriging Generative Seismic Kriging with Normalizing Flows\nDetectability with Vision Classification Analysis of CO2 Leakage Detectability via Vision Classifier\n3D survey design 3D seismic survey design by maximizing the spectral gap"
  },
  {
    "objectID": "DetectabilityWithVision/abstract.html",
    "href": "DetectabilityWithVision/abstract.html",
    "title": "Analysis of CO2 Leakage Detectability via Vision Classifier",
    "section": "",
    "text": "succesful detection\nneeds improvement and degrades\naugment dataset\nto improve results\n\nGlobal deployment of geological carbon storage (GCS) technology hinges on alleviating possible risk factors such as reservoir leakage. Reservoir leakage can successfully be observed with seismic imaging and the leakage detection process can be automated with machine learning. However, the performance of the trained networks is contingent on building a dataset for each new receiver configuration in a seismic experiment, leading to meticulous and high-cost data collection/generation procedures. In this study, we aim to improve the detection generalization of a network, trained on only a particular seismic receiver configuration, by proposing an augmentation to its training dataset."
  },
  {
    "objectID": "DetectabilityWithVision/abstract.html#methods-procedures-process-250-words",
    "href": "DetectabilityWithVision/abstract.html#methods-procedures-process-250-words",
    "title": "Analysis of CO2 Leakage Detectability via Vision Classifier",
    "section": "Methods, Procedures, Process (250 words)",
    "text": "Methods, Procedures, Process (250 words)\nThe dataset generation procedure commences with multiple 2D proxy velocity models constructed as a representative of a potential GCS site in the South of the North Sea. To train the classification networks, we create a training set of seismic images of CO2 plumes that behave regularly (without leakage) and irregularly (with leakage). In this approach, we make use of a rock physics model to convert time-varying CO2 concentrations in the reservoir to variations in the acoustic wavespeed that we image with reverse time migration (RTM). The CO2 plumes themselves are modeled by the two-phase flow equations in an assumed to be known model for permeability. we observe the time-lapse difference between baseline and monitor images. RTM utilizes equally spaced 162 receivers and jittered 32 sources configuration. The resulting RTM time-lapse images is used to form training dataset of network, based on Vision Transformer. The ultimate purpose of the network is to distinguish between regular and irregular seismic images of reservoir by performing binary classification and the overall training workflow can be seen in Figure 1(a).\nTo test the network’s performance in different receiver configurations, we reiterate the RTM with different number of receivers placed periodically. The generated time-lapse difference images form the test set. As shown in Figure 1(b), 200 receiver configuration produces high-quality, artifact-free reservoir difference images, whereas for 20 receiver configuration setting the reservoir is very noisy. Therefore, before performing the testing experiment, we hypothesize that the network trained on one specific receiver configuration may face performance issues on images generated by different configurations."
  },
  {
    "objectID": "DetectabilityWithVision/abstract.html#results-observations-conclusions-250-words",
    "href": "DetectabilityWithVision/abstract.html#results-observations-conclusions-250-words",
    "title": "Analysis of CO2 Leakage Detectability via Vision Classifier",
    "section": "Results, Observations, Conclusions (250 words)",
    "text": "Results, Observations, Conclusions (250 words)\nThe testing accuracy results of the network, trained initially on 162 receiver setting, is shown in Figure 2(a). It can be seen that the detection accuracy of the model is significantly high in receiver configurations more than 162 receivers and gradually decreases in the following configurations. After approximately 100 receivers, the binary classification results become equivalent to random.\nTo uplift the general accuracy performance of the network, we enlarge the training dataset by including examples of difference images produced by random receiver configurations. Considering the fact that dataset augmentation introduces a new cost to our framework, we specifically included random, low-cost examples with number of receivers less than 162. After the dataset augmentation process, we can observe that the classification accuracy for all configurations has been significantly increased, shown in Figure(2a). Moreover, the confusion matrix performance metrics(precision, recall and F-1 score) have been substantially improved.\nThe results show that dataset augmentation can enhance the generalization of network and contribute to its detection performance. In terms of practical use, this work suggests a dataset based on a particular seismic setting can be made more applicable to different imaging experiments with the inclusion of cheap random examples in the training dataset."
  },
  {
    "objectID": "DetectabilityWithVision/abstract.html#significancenovelty-100-words",
    "href": "DetectabilityWithVision/abstract.html#significancenovelty-100-words",
    "title": "Analysis of CO2 Leakage Detectability via Vision Classifier",
    "section": "Significance/Novelty (100 words)",
    "text": "Significance/Novelty (100 words)\nTo our knowledge, our study is the first numerical study to frame the problem of detectability of seismic CO2 leakage images in GCS reservoirs using a ML based network model. Additionally, the study proposes a dataset augmentation technique to improve the generalization of the network to scenarios with different acquisition settings, notably different number of receivers. The results suggest that our study has the potential to lower the cost associated with dataset collection, create networks applicable to general seismic imaging schemes and expedite the leakage detection process with an automatic workflow."
  }
]